{"ast":null,"code":"/**\n * Copyright 2015 IBM Corp. All Rights Reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n'use strict';\n\nvar getUserMedia = require('get-user-media-promise');\n\nvar MicrophoneStream = require('microphone-stream');\n\nvar RecognizeStream = require('./recognize-stream.js');\n\nvar L16 = require('./webaudio-l16-stream.js');\n\nvar FormatStream = require('./format-stream.js');\n\nvar assign = require('object.assign/polyfill')();\n\nvar WritableElementStream = require('./writable-element-stream');\n\nvar _require = require('readable-stream'),\n    Writable = _require.Writable;\n\nvar ResultStream = require('./result-stream');\n\nvar SpeakerStream = require('./speaker-stream');\n\nvar preservedMicStream;\nvar bitBucket = new Writable({\n  write: function (chunk, encoding, callback) {\n    // when the keepMicrophone option is enabled, unused audio data is sent here so that it isn't buffered by other streams.\n    callback();\n  },\n  objectMode: true,\n  // can still accept strings/buffers\n  decodeStrings: false\n});\n/**\n * @module watson-speech/speech-to-text/recognize-microphone\n */\n\n/**\n * Create and return a RecognizeStream sourcing audio from the user's microphone\n *\n * @param {Object} options - Also passed to {RecognizeStream}, and {FormatStream} when applicable\n * @param {String} options.token - Auth Token for CF services - see https://github.com/watson-developer-cloud/node-sdk#authorization\n * @param {String} options.access_token - IAM Access Token for RC services - see https://github.com/watson-developer-cloud/node-sdk#authorization\n * @param {String} [options.url='wss://stream.watsonplatform.net/speech-to-text/api'] - Base URL for a service instance\n * @param {Boolean} [options.format=true] - pipe the text through a FormatStream which performs light formatting. Also controls smart_formatting option unless explicitly set.\n * @param {Boolean} [options.keepMicrophone=false] - keeps an internal reference to the microphone stream to reuse in subsequent calls (prevents multiple permissions dialogs in firefox)\n * @param {String|DOMElement} [options.outputElement] pipe the text to a [WriteableElementStream](WritableElementStream.html) targeting the specified element. Also defaults objectMode to true to enable interim results.\n * @param {Boolean} [options.extractResults=false] pipe results through a ResultStream stream to simplify the objects. (Default behavior before v0.22) Requires objectMode.\n * @param {Boolean} [options.resultsBySpeaker=false] Pipe results through a SpeakerStream. Forces speaker_labels and objectMode to be true.\n * @param {MediaStream} [options.mediaStream] Optionally pass in an existing MediaStream\n *\n * @return {RecognizeStream|SpeakerStream|FormatStream|ResultStream}\n */\n\nmodule.exports = function recognizeMicrophone(options) {\n  if (!options || !options.token && !options.access_token) {\n    throw new Error('WatsonSpeechToText: missing required parameter: opts.token (CF) or opts.access_token (RC)');\n  } // the WritableElementStream works best in objectMode\n\n\n  if (options.outputElement && options.objectMode !== false) {\n    options.objectMode = true;\n  } // the ResultExtractor only works in objectMode\n\n\n  if (options.extractResults) {\n    options.objectMode = true;\n  } // SpeakerStream requires objectMode and speaker_labels\n\n\n  if (options.resultsBySpeaker) {\n    options.objectMode = true;\n    options.speaker_labels = true;\n  } // default format to true (capitals and periods)\n  // default smart_formatting to options.format value (dates, currency, etc.)\n\n\n  options.format = options.format !== false;\n\n  if (typeof options.smart_formatting === 'undefined') {\n    options.smart_formatting = options.format;\n  }\n\n  var rsOpts = assign({\n    'content-type': 'audio/l16;rate=16000',\n    interim_results: true\n  }, options);\n  var recognizeStream = new RecognizeStream(rsOpts);\n  var streams = [recognizeStream]; // collect all of the streams so that we can bundle up errors and send them to the last one\n  // set up the output first so that we have a place to emit errors\n  // if there's trouble with the input stream\n\n  var stream = recognizeStream;\n  var keepMic = options.keepMicrophone;\n  var micStream;\n\n  if (keepMic && preservedMicStream) {\n    preservedMicStream.unpipe(bitBucket);\n    micStream = preservedMicStream;\n  } else {\n    // create the MicrophoneStream synchronously to allow it to resume the context in Safari on iOS 11\n    micStream = new MicrophoneStream({\n      objectMode: true,\n      bufferSize: options.bufferSize\n    });\n    var pm = options.mediaStream ? Promise.resolve(options.mediaStream) : getUserMedia({\n      video: false,\n      audio: true\n    });\n    pm.then(function (mediaStream) {\n      micStream.setStream(mediaStream);\n\n      if (keepMic) {\n        preservedMicStream = micStream;\n      }\n    }).catch(function (err) {\n      stream.emit('error', err);\n\n      if (err.name === 'NotSupportedError') {\n        stream.end(); // end the stream\n      }\n    });\n  }\n\n  var l16Stream = new L16({\n    writableObjectMode: true\n  });\n  micStream.pipe(l16Stream).pipe(recognizeStream);\n  streams.push(micStream, l16Stream);\n  /**\n   * unpipes the mic stream to prevent any more audio from being sent over the wire\n   * temporarily re-pipes it to the bitBucket (basically /dev/null)  becuse\n   * otherwise it will buffer the audio from in between calls and prepend it to the next one\n   *\n   * @private\n   */\n\n  function end() {\n    micStream.unpipe(l16Stream);\n    micStream.pipe(bitBucket);\n    l16Stream.end();\n  } // trigger on both stop and end events:\n  // stop will not fire when a stream ends due to a timeout\n  // but when stop does fire, we want to honor it immediately\n  // end will always fire, but it may take a few moments after stop\n\n\n  if (keepMic) {\n    recognizeStream.on('end', end);\n    recognizeStream.on('stop', end);\n  } else {\n    recognizeStream.on('end', micStream.stop.bind(micStream));\n    recognizeStream.on('stop', micStream.stop.bind(micStream));\n  }\n\n  if (options.resultsBySpeaker) {\n    stream = stream.pipe(new SpeakerStream(options));\n    streams.push(stream);\n  }\n\n  if (options.format) {\n    stream = stream.pipe(new FormatStream(options));\n    streams.push(stream);\n  }\n\n  if (options.outputElement) {\n    // we don't want to return the WES, just send data to it\n    streams.push(stream.pipe(new WritableElementStream(options)));\n  }\n\n  if (options.extractResults) {\n    stream = stream.pipe(new ResultStream());\n    streams.push(stream);\n  } // Capture errors from any stream except the last one and emit them on the last one\n\n\n  streams.forEach(function (prevStream) {\n    if (prevStream !== stream) {\n      prevStream.on('error', stream.emit.bind(stream, 'error'));\n    }\n  });\n\n  if (stream !== recognizeStream) {\n    // add a stop button to whatever the final stream ends up being\n    stream.stop = recognizeStream.stop.bind(recognizeStream);\n  } // expose the original stream to for debugging (and to support the JSON tab on the STT demo)\n\n\n  stream.recognizeStream = recognizeStream;\n  return stream;\n};\n\nmodule.exports.isSupported = getUserMedia.isSupported;","map":null,"metadata":{},"sourceType":"script"}